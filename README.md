# Unintended Memorization of Sensitive Information in Fine-Tuned Language Models
**Accepted to EACL 2026**

This repository contains the official implementation for the paper **"Unintended Memorization of Sensitive Information in Fine-Tuned Language Models"**.

### TL;DR
We analyze how fine-tuned LLMs unintentionally memorize sensitive data (PII) present in input prompts. We benchmark four mitigation strategiesâ€”**Differential Privacy, Machine Unlearning, Regularization, and Preference Alignment (DPO)**â€”evaluating their trade-offs between privacy protection and model utility on both real and synthetic datasets.

<p align="center">
  <img src="media/teaser.png" alt="Teaser Figure" width="700">
</p>



## ğŸ“Œ Abstract
Fine-tuning Large Language Models (LLMs) on sensitive datasets carries a substantial risk of unintended memorization and leakage of Personally Identifiable Information (PII), which can violate privacy regulations and compromise individual safety. In this work, we systematically investigate a critical and underexplored vulnerability: the exposure of PII that appears only in model inputs, not in training targets. Using both synthetic and real-world datasets, we design controlled extraction probes to quantify unintended memorization and study how factors such as language, PII frequency, task type, and model size influence memorization behavior. We further benchmark four privacy-preserving approaches including differential privacy, machine unlearning, regularization, and preference alignment, evaluating their trade-offs between privacy and task performance. Our results show that post-training methods generally provide more consistent privacyâ€“utility trade-offs, while differential privacy achieves strong reduction in leakage in specific settings, although it can introduce training instability. These findings highlight the persistent challenge of memorization in fine-tuned LLMs and emphasize the need for robust, scalable privacy-preserving techniques.


## ğŸ“ Repository Structure

The codebase is organized into modular components for training, attacks, and evaluation:

```
src/
â”œâ”€â”€ Attacks/       # PII Extraction Probes (True Prefix Attack)
â”‚   â”œâ”€â”€ true-prefix-gretel.py
â”‚   â””â”€â”€ true-prefix-others.py
â”œâ”€â”€ DP-FT/         # Differential Privacy Fine-Tuning
â”‚   â”œâ”€â”€ dp-disch-summ.py
â”‚   â”œâ”€â”€ dp-Gretel.py
â”‚   â””â”€â”€ DP-patho.py
â”œâ”€â”€ DPO/           # Direct Preference Optimization
â”‚   â””â”€â”€ dirprefopt.py
â”œâ”€â”€ SFT/           # Standard Supervised Fine-Tuning
â”‚   â”œâ”€â”€ fine-tune-disch-summ.py
â”‚   â”œâ”€â”€ fine-tune-gretel.py
â”‚   â””â”€â”€ fine-tune-pathology.py
â”œâ”€â”€ Task-Eval/     # Utility Evaluation Scripts
â”‚   â”œâ”€â”€ eval-disch-summ.py
â”‚   â”œâ”€â”€ eval-gretel.py
â”‚   â”œâ”€â”€ eval-pathology.py
â”‚   â””â”€â”€ log-likelihood-disch-summ.py
â”œâ”€â”€ undial/        # Machine Unlearning / Undial Implementation
â”‚   â””â”€â”€ main_undial.py
â””â”€â”€ utils/         # Helper functions
```
### ğŸ”§ Utility Modules

**`src/utils/`**
- `TumorClassification.py`: Classes related to the evaluation of the Pathology task using structured outputs.

### ğŸ” Machine Unlearning (UnDial)

**`src/undial/`** implements machine unlearning and alternating regularization techniques based on the original UnDial repository:

https://github.com/dong-river/LLM_unlearning/

Most utility code in `undial/utils` is adapted from the original implementation, with additional classes added in `data_utils` to support the structure and format of the datasets used in this project.

For a complete description of supported arguments, configurations, and advanced implementation details, please refer to the original UnDial repository.

- **`notebooks/`**: Contains Jupyter notebooks for dataset creation, result compilation, and visualization (e.g., `gretel_results.ipynb`, `result_compilation.ipynb`).

## ğŸš€ Installation

We recommend using Python 3.10+ and creating a virtual environment.

```bash
# Clone the repository
git clone https://github.com/your-org/llm-pii-leak.git
cd llm-pii-leak

# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt
```


## ğŸ› ï¸ Usage

### 1. Supervised Fine-Tuning (SFT)
Base baselines are established using standard SFT.
```bash
python src/SFT/fine-tune-gretel.py
```

### 2. Privacy-Preserving Methods
We support multiple mitigation strategies:

*   **Differential Privacy (DP-SGD):**
    ```bash
    python src/DP-FT/dp-Gretel.py
    ```

*   **Direct Preference Optimization (DPO):**
    ```bash
    python src/DPO/dirprefopt.py
    ```

*   **Machine Unlearning:**
    ```bash
    python src/undial/main_undial.py [arguments]
    ```

### 3. Attacks & Evaluation
*   **Run PII Extraction Attacks (True Prefix):**
    ```bash
    python src/Attacks/true-prefix-gretel.py
    ```

*   **Evaluate Model Utility:**
    ```bash
    python src/Task-Eval/eval-gretel.py
    ```


## ğŸ“œ Citation

If you use this code or find our work helpful, please cite our paper:

```bibtex
@misc{szep_unintended_2026,
  title = {Unintended {Memorization} of {Sensitive} {Information} in {Fine}-{Tuned} {Language} {Models}},
  author = {Szep, Marton and Ruiz, Jorge Marin and Kaissis, Georgios and Seidl, Paulina and Eisenhart-Rothe, RÃ¼diger von and Hinterwimmer, Florian and Rueckert, Daniel},
  url = {http://arxiv.org/abs/2601.17480},
  doi = {10.48550/arXiv.2601.17480},
  urldate = {2026-01-29},
  publisher = {arXiv},
  month = jan,
  year = {2026},
  eprint={2601.17480},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  note = {arXiv:2601.17480 [cs]},
  annote = {Comment: Accepted to EACL 2026. 20 pages},
}
```


## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.