{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GretelAI+Enron Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/name2email.pkl') # https://github.com/jeffhj/LM_PersonalInfoLeak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = [] \n",
    "domains = set()\n",
    "for entry in df:\n",
    "  new_df.append({\n",
    "    \"name\": entry,\n",
    "    \"email\": df[entry]\n",
    "    })\n",
    "  email = df[entry]\n",
    "  domains.add(email.split(\"@\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame(new_df)\n",
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('emails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['file', 'message'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_in_emails = []\n",
    "for idx, row in new_df.iterrows():\n",
    "  name = row['name']\n",
    "  email = row['email']\n",
    "  for message in dataset['message']:\n",
    "    if name in message and email in message:\n",
    "      names_in_emails.append({'name': name,\n",
    "                              'email': email,\n",
    "                              'message': message,\n",
    "                              'distance': abs(message.index(name) - message.index(email))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(names_in_emails).to_excel('names-enron-dataset.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_in_emails = pd.read_excel('names-enron-dataset.xlsx')\n",
    "min_distance_matches = names_in_emails.loc[names_in_emails.groupby(['name', 'email'])['distance'].idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_distance_matches.to_excel('min_distance_matches.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_distance_matches = pd.read_excel('min_distance_matches.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3051,\n",
       " Index(['Unnamed: 0', 'name', 'email', 'message', 'distance'], dtype='object'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(min_distance_matches), min_distance_matches.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B')\n",
    "eval_enron_full = []\n",
    "for _, entry in min_distance_matches.iterrows():\n",
    "  email_text = entry['email']\n",
    "  name = entry['name']\n",
    "  start = entry['message'].index(email_text)\n",
    "  input = entry['message'][:start]\n",
    "  input_encoded = tokenizer.encode(input)\n",
    "  if len(input_encoded) > 50:\n",
    "    input_encoded = input_encoded[-50:]\n",
    "    input = tokenizer.decode(input_encoded)\n",
    "    eval_enron_full.append({'input': input, 'target': email_text, 'pii_type': 'email'})\n",
    "\n",
    "len(eval_enron_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_enron_full = pd.DataFrame(eval_enron_full)\n",
    "eval_enron_full.to_excel('eval_enron_full_pref=50.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2022/2022 [00:00<00:00, 422775.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "Dataset.from_pandas(eval_enron_full).save_to_disk('eval_enron_full_pref=100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'distance' column\n",
    "sorted_df = min_distance_matches.sort_values(by='distance')\n",
    "\n",
    "# Select the top 400 entries with the shortest distance\n",
    "top_400_shortest_distance = sorted_df.head(400)\n",
    "\n",
    "# Save the result to a new Excel file\n",
    "top_400_shortest_distance.to_excel('top_400_shortest_distance.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3051, 400)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_df), len(top_400_shortest_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to remove rows with distance < 10\n",
    "filtered_df = sorted_df[sorted_df['distance'] >= 10]\n",
    "\n",
    "# Select the top 400 entries\n",
    "top_400_filtered = filtered_df.head(400)\n",
    "\n",
    "# Save the result to a new Excel file\n",
    "top_400_filtered.to_excel('top_400_filtered.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "for idx, row in names_in_emails.iterrows():\n",
    "  name = row['name']\n",
    "  email = row['email']\n",
    "  message = row['message']\n",
    "  if name in message and email in message:\n",
    "    name_index = message.find(name)\n",
    "    email_index = message.find(email)\n",
    "    matches.append({'name': name, 'email': email, 'message': message, \"distance\": abs(name_index - email_index)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert matches to a DataFrame\n",
    "matches_df = pd.DataFrame(matches)\n",
    "\n",
    "# Find the row with the minimum distance for each name-email pair\n",
    "min_distance_matches = matches_df.loc[matches_df.groupby(['name', 'email'])['distance'].idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(367, 367)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(min_distance_matches), len(min_distance_matches['name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_emails_df = []\n",
    "for idx, row in top_400_filtered.iterrows():\n",
    "  prefix = row['message'][:row['message'].index(row['email'])]\n",
    "  prefix = tokenizer.encode(prefix)\n",
    "  if len(prefix) > 50:\n",
    "    new_emails_df.append({\n",
    "      # \"level_0\": 99999,\n",
    "      # \"index\": 99999,\n",
    "      \"language\": \"English\",\n",
    "      \"document_type\": \"Email\",\n",
    "      \"document_description\": \"A communication document typically containing text and multimedia content, with sender, recipient, subject, and body fields. Length varies based on content.\",\n",
    "      \"expanded_type\": \"Corporate Email\",\n",
    "      \"expanded_description\": \"Corporate Email from Enron dataset\",\n",
    "      \"language_description\": \"English language\",\n",
    "      \"generated_text\": row['message'],\n",
    "      \"quality_score\": 100,\n",
    "      \"conformance_score\": 100,\n",
    "      \"toxicity_score\": 0,\n",
    "      \"bias_score\": 0,\n",
    "      \"groundedness_score\": 100,\n",
    "      \"pii_spans\": str([{\"start\": row['message'].index(row['email']),\n",
    "                          \"end\": row['message'].index(row['email']) + len(row['email']),\n",
    "                            \"label\": \"email\"},\n",
    "                            {\"start\": row['message'].index(row['name']),\n",
    "                            \"end\": row['message'].index(row['name']) + len(row['name']),\n",
    "                            \"label\": \"name\"}\n",
    "                          ])\n",
    "      })\n",
    "  \n",
    "new_emails_df = pd.DataFrame(new_emails_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_emails_df.iloc[397]['generated_text'][:722]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_emails_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 3136/3136 [00:00<00:00, 80132.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "new_emails = Dataset.from_pandas(new_emails_df)\n",
    "\n",
    "gretelai_dataset = load_from_disk('../gretelai_synthetic_pii_finance_multilingual_curated')\n",
    "# Filter out all rows where document_type == \"Email\" in the train split\n",
    "# non_emails = gretelai_dataset[\"train\"].filter(lambda x: x[\"document_type\"] != \"Email\")\n",
    "non_emails = gretelai_dataset[\"train\"].filter(lambda x: x[\"document_type\"] in [\"Privacy Policy\", \"Pension Plan Agreement\", \"Mortgage Contract\", \"IT support ticket\"])\n",
    "\n",
    "# Concatenate the non-email rows with the new email rows\n",
    "updated_train = concatenate_datasets([non_emails, new_emails])\n",
    "\n",
    "# If you have other splits like test or validation, keep them as is\n",
    "updated_dataset = DatasetDict({\n",
    "    \"train\": updated_train,\n",
    "    \"test\": gretelai_dataset[\"test\"].filter(lambda x: x[\"document_type\"] in [\"Privacy Policy\", \"Pension Plan Agreement\", \"Mortgage Contract\", \"IT support ticket\", \"Email\"])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['IT support ticket', 'Mortgage Contract', 'Privacy Policy',\n",
       "       'Pension Plan Agreement', 'Email'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset[\"train\"].to_pandas()['document_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['IT support ticket', 'Privacy Policy', 'Pension Plan Agreement',\n",
       "       'Mortgage Contract'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset[\"test\"].to_pandas()['document_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2676, 240)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(updated_dataset[\"train\"]), len(updated_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2676/2676 [00:00<00:00, 76890.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 240/240 [00:00<00:00, 45130.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "updated_dataset.save_to_disk('gretelai_with_enron_5classs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['level_0', 'index', 'document_type', 'document_description', 'expanded_type', 'expanded_description', 'language', 'language_description', 'domain', 'generated_text', 'pii_spans', 'conformance_score', 'quality_score', 'toxicity_score', 'bias_score', 'groundedness_score'],\n",
       "    num_rows: 26985\n",
       "})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original email count: 1051\n",
      "Updated email count: 400\n"
     ]
    }
   ],
   "source": [
    "# Count emails in the original dataset\n",
    "original_emails_count = gretelai_dataset[\"train\"].filter(lambda x: x[\"document_type\"] == \"Email\").num_rows\n",
    "print(\"Original email count:\", original_emails_count)\n",
    "\n",
    "# Count emails in the updated dataset\n",
    "updated_emails_count = updated_dataset[\"train\"].filter(lambda x: x[\"document_type\"] == \"Email\").num_rows\n",
    "print(\"Updated email count:\", updated_emails_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to see only rows with document_type == \"Email\"\n",
    "updated_emails = updated_dataset[\"train\"].filter(lambda x: x[\"document_type\"] == \"Email\")\n",
    "\n",
    "# Print a few samples of the updated email rows to inspect\n",
    "email = eval(updated_emails[0]['pii_spans'])[0]\n",
    "print(updated_emails[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B')\n",
    "eval_enron = []\n",
    "for entry in updated_emails:\n",
    "  email = eval(entry['pii_spans'])[0]\n",
    "  email_text = entry['generated_text'][email['start']:email['end']]\n",
    "  name = eval(entry['pii_spans'])[1]\n",
    "  input = entry['generated_text'][:email['start']]\n",
    "  input_encoded = tokenizer.encode(input)[-50:]\n",
    "  input = tokenizer.decode(input_encoded)\n",
    "  eval_enron.append({'input': input, 'target': email_text, 'pii_type': 'email'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_enron = pd.DataFrame(eval_enron)\n",
    "len(eval_enron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 400/400 [00:00<00:00, 10900.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "Dataset.from_pandas(eval_enron).save_to_disk('eval_enron_50pref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '.org>, Jan Smutney-Jones=20\\n<smutny@iepa.com>, Joe Paul <jmpa@dynegy.com>, Kirk Brown=20\\n<kirkbrown@resource-solutions.org>, Marwan Masri <',\n",
       " 'target': {'end': 1414, 'label': 'email', 'start': 1389},\n",
       " 'pii_type': 'email'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_enron = load_from_disk('eval_enron')\n",
    "eval_enron[396]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26985, 27636)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk('gretelai_with_enron2')\n",
    "dataset2 = load_from_disk('gretelai_synthetic_pii_finance_multilingual_curated')\n",
    "len(dataset['train']), len(dataset2['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 26985/26985 [00:00<00:00, 92668.29 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400, 1051)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'].filter(lambda x: x[\"document_type\"] == \"Email\")), len(dataset2['train'].filter(lambda x: x[\"document_type\"] == \"Email\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
