{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33849805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from re import sub\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6074f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac8f166",
   "metadata": {},
   "source": [
    "## GRETELAI - 1st approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1839f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['level_0',\n",
       "  'index',\n",
       "  'document_type',\n",
       "  'document_description',\n",
       "  'expanded_type',\n",
       "  'expanded_description',\n",
       "  'language',\n",
       "  'language_description',\n",
       "  'domain',\n",
       "  'generated_text',\n",
       "  'pii_spans',\n",
       "  'conformance_score',\n",
       "  'quality_score',\n",
       "  'toxicity_score',\n",
       "  'bias_score',\n",
       "  'groundedness_score'],\n",
       " 27636)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk('datasets/gretelai_synthetic_pii_finance_multilingual_curated')\n",
    "dataset['train'].column_names, len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce490aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 119, 'end': 141, 'label': 'date'},\n",
       " {'start': 181, 'end': 197, 'label': 'company'},\n",
       " {'start': 305, 'end': 333, 'label': 'street_address'},\n",
       " {'start': 363, 'end': 379, 'label': 'company'},\n",
       " {'start': 386, 'end': 399, 'label': 'name'},\n",
       " {'start': 441, 'end': 469, 'label': 'street_address'},\n",
       " {'start': 481, 'end': 497, 'label': 'company'},\n",
       " {'start': 598, 'end': 614, 'label': 'company'},\n",
       " {'start': 709, 'end': 725, 'label': 'company'},\n",
       " {'start': 915, 'end': 931, 'label': 'company'},\n",
       " {'start': 1026, 'end': 1042, 'label': 'company'},\n",
       " {'start': 1797, 'end': 1813, 'label': 'company'},\n",
       " {'start': 1923, 'end': 1939, 'label': 'company'},\n",
       " {'start': 1283, 'end': 1295, 'label': 'date'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(dataset['train'][0]['pii_spans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc896b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(candidate_labels):\n",
    "  prompt = (\n",
    "    f\"Classify the following text into one of the following labels and mask all PII (Personally Identifiable Information). \\n\"\n",
    "    f\"Possible labels: {', '.join(candidate_labels)}\\n\"\n",
    "  )\n",
    "  return prompt\n",
    "\n",
    "def generate_label(text, label):\n",
    "  prompt = (\n",
    "    f\"Text:\\n{text}\\n\"\n",
    "    f\"Document type: {label}\"\n",
    "  )\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a18ca60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "prompt = generate_prompt(set(dataset['train']['document_type']))\n",
    "for i in range(10000):\n",
    "  sample = {}\n",
    "  piis = {}\n",
    "  text = dataset['train'][i]['generated_text']\n",
    "  for pii in json.loads(dataset['train'][i]['pii_spans']):\n",
    "    pii_text = text[pii['start']:pii['end']]\n",
    "    piis[pii_text] = pii['label'].upper()\n",
    "  for pii in piis.keys():\n",
    "    text = text.replace(pii, f'<{piis[pii]}>')\n",
    "  \n",
    "  sample['prompt'] = prompt\n",
    "  sample['chosen'] = generate_label(text, dataset['train'][i]['document_type'])\n",
    "  sample['rejected'] = generate_label(dataset['train'][i]['generated_text'], dataset['train'][i]['document_type'])\n",
    "  new_dataset.append(sample)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89e3c999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"Classify the following text into one of the following labels and mask all PII (Personally Identifiable Information). \\nPossible labels: Employment Contract, Financial Data Feed, Credit Application, Investment Prospectus, Tax Return, Mortgage Contract, Bank Statement, Health Insurance Claim Form, Customer support conversational log, Pension Plan Agreement, Audit Report, Corporate Tax Return, IT support ticket, Financial Risk Assessment, Loan Application, Corporate Governance Guidelines, Customer Agreement, Credit Card Statement, Financial Aid Application, Financial Regulatory Compliance Report, Securities Prospectus, Insurance Claim Form, Compliance Certificate, Trade Confirmation, Financial Disclosure Statement, Renewal Reminder, Policyholder's Report, Real Estate Loan Agreement, Mortgage Amortization Schedule, Bill of Lading, Business Plan, Dispute Resolution Policy, Email, Safety Data Sheet, Credit Card Application, Privacy Policy, Financial Forecast, Annual Report, Transaction Confirmation, Supply Chain Management Agreement, Shareholder Agreement, Financial Statement, Insurance Policy, Currency Exchange Rate Sheet, Loan Agreement, Product Disclosure Statement, Regulatory Filing, ISDA Definition, Regulatory Compliance Guide, Cryptocurrency Transaction Report, Tax Assessment Notice, Payment Confirmation\\n\",\n",
       " 'chosen': \"Text:\\n------------------------------------------------------------------------------------------------------------------------\\nCustomer Support Conversational Log\\n------------------------------------------------------------------------------------------------------------------------\\n\\nTimestamp: <DATE> <TIME>\\nCustomer ID: <CUSTOMER_ID>\\nFirst Name: <FIRST_NAME>\\n\\nAgent: Hello <FIRST_NAME>, welcome to our Warranty Claims department. How may I assist you <DATE>?\\n\\nTimestamp: <DATE> <TIME>\\nCustomer ID: <CUSTOMER_ID>\\nFirst Name: <FIRST_NAME>\\n\\n<FIRST_NAME>: Hi, I have a product that I would like to make a warranty claim for. It's a <NAME> blender that I purchased <DATE>.\\n\\nAgent: I'm sorry to hear that you're having issues with your blender, <FIRST_NAME>. To better assist you, could you please provide me with the street address associated with the purchase?\\n\\nTimestamp: <DATE> <TIME>\\nCustomer ID: <CUSTOMER_ID>\\nFirst Name: <FIRST_NAME>\\nName: <NAME>\\nStreet Address: <STREET_ADDRESS>\\n\\n<FIRST_NAME>: Sure, it's <STREET_ADDRESS>.\\n\\nAgent: Thank you, <FIRST_NAME>. I see that you have a valid warranty for the product. I'll guide you through the process of filing a claim. To begin, could you please provide me with the serial number of the blender?\\n\\n------------------------------------------------------------------------------------------------------------------------\\nDocument type: Customer support conversational log\",\n",
       " 'rejected': \"Text:\\n------------------------------------------------------------------------------------------------------------------------\\nCustomer Support Conversational Log\\n------------------------------------------------------------------------------------------------------------------------\\n\\nTimestamp: 2022-03-15 10:05:00\\nCustomer ID: J292827-Yx\\nFirst Name: Amy\\n\\nAgent: Hello Amy, welcome to our Warranty Claims department. How may I assist you today?\\n\\nTimestamp: 2022-03-15 10:06:15\\nCustomer ID: J292827-Yx\\nFirst Name: Amy\\n\\nAmy: Hi, I have a product that I would like to make a warranty claim for. It's a Luciano Fittipaldi blender that I purchased a few months ago.\\n\\nAgent: I'm sorry to hear that you're having issues with your blender, Amy. To better assist you, could you please provide me with the street address associated with the purchase?\\n\\nTimestamp: 2022-03-15 10:07:30\\nCustomer ID: J292827-Yx\\nFirst Name: Amy\\nName: Luciano Fittipaldi\\nStreet Address: 76452 Rodriguez Parks, Suite 432\\n\\nAmy: Sure, it's 76452 Rodriguez Parks, Suite 432.\\n\\nAgent: Thank you, Amy. I see that you have a valid warranty for the product. I'll guide you through the process of filing a claim. To begin, could you please provide me with the serial number of the blender?\\n\\n------------------------------------------------------------------------------------------------------------------------\\nDocument type: Customer support conversational log\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a238adc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Classify the following text into one of the fo...</td>\n",
       "      <td>Text:\\nSUPPLY CHAIN MANAGEMENT AGREEMENT\\n\\nTh...</td>\n",
       "      <td>Text:\\nSUPPLY CHAIN MANAGEMENT AGREEMENT\\n\\nTh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classify the following text into one of the fo...</td>\n",
       "      <td>Text:\\nCONTRATO DE PRÉSTAMO PARA INVERSIÓN INM...</td>\n",
       "      <td>Text:\\nCONTRATO DE PRÉSTAMO PARA INVERSIÓN INM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Classify the following text into one of the fo...   \n",
       "1  Classify the following text into one of the fo...   \n",
       "\n",
       "                                              chosen  \\\n",
       "0  Text:\\nSUPPLY CHAIN MANAGEMENT AGREEMENT\\n\\nTh...   \n",
       "1  Text:\\nCONTRATO DE PRÉSTAMO PARA INVERSIÓN INM...   \n",
       "\n",
       "                                            rejected  \n",
       "0  Text:\\nSUPPLY CHAIN MANAGEMENT AGREEMENT\\n\\nTh...  \n",
       "1  Text:\\nCONTRATO DE PRÉSTAMO PARA INVERSIÓN INM...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.DataFrame(new_dataset)\n",
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e00af3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (9000, 3)\n",
      "Validation set shape: (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "# create train / validation split for the comparison dataframe\n",
    "train_df, val_df = train_test_split(dataset, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "print(f\"Train set shape: {train_df.shape}\")\n",
    "print(f\"Validation set shape: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d6086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 9000/9000 [00:00<00:00, 668000.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 396324.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dpo_dataset = DatasetDict({\n",
    "  'train': Dataset.from_pandas(train_df),\n",
    "  'validation': Dataset.from_pandas(val_df)\n",
    "})\n",
    "\n",
    "dpo_dataset.save_to_disk('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48fbcdd",
   "metadata": {},
   "source": [
    "## Sliding Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a319b0",
   "metadata": {},
   "source": [
    "Discharge Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567d227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21044,\n",
       " ['report', 'file', 'id', 'piis', 'label', 'input_text', '__index_level_0__'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk('')\n",
    "len(dataset['train']), dataset['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ad9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]['piis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44924d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list included other common terms such as City names or locations. PIIs have been removed for privacy.\n",
    "stop_terms = set(term.lower() for term in [\n",
    "  \"vor.\", \"sich\", \"des\", \"Wochen\", \"langen\", \"zur\", \"Schmerzen\", \"Straße\", \"Lunge\", \"Str.\", \n",
    "  \"befand.\", \"Becken\", \"Bereich\", \"Reich\", \"Therapie\", \"einem\", \"Dank\", \"Seite\", \"Schulter\", \n",
    "  \"RECH\", \"Rech\", \"Praxis\", \"Meter\", \"Alt\", \"März\", \"Sohn\", \"Hand\", \"Schule\", \"Eltern\", \n",
    "  \"Unfallchirurgie\", \"Befundes\", \"Kernspintomographie\", \"Procedere\", \"Montag\", \"Befund\", \n",
    "  \"Freund\", \"Liebe\", \"Ganz\", \"GANZ\", \"St.\", \"Ihnen\", \"Orthop.\", \"Sportorthop.\", \"Then\", \n",
    "  \"Orth\", \"Orth.\", \"Junge.\", \"Jahres\", \"List\", \"Danke\", \"Länge\", \"seh\", \"beidseits.\", \n",
    "  \"Humerusfraktur\", \"Skisturzes\", \"L5/S1\", \"ambulanter\", \"Ellenbogen\", \"medial\", \"FREUND\", \n",
    "  \"Entz\", \"Streck\", \"MEHR\", \"Schuh\", \"Voll\", \"Berichten\", \"Beschwerden\", \"Hausarzt\", \"Kraft\", \n",
    "  \"Führt\", \"Hause\", \"Regel\", \"voll\", \"Kurz\", \"Heim\", \"links.\", \"besten\", \"Sch.\", \" v.a.\", \n",
    "  \"Klinik\", \"Kliniken\", \"Klinikums\", \"Klinikum\", \"schwer\", \"Hoch\", \"Lang\", \"Weil\", \"Länger\", \n",
    "  \"res.\", \"rein\", \"Sonntag\", \"dürfen\", \"Stat.\", \"GRUND\", \"rascher\", \"Pantozol\", \"Oxygesic\", \n",
    "  \"Fragmin\", \"Chirurgie\", \"Osteosynthese\", \"Stock\", \"Paar\", \"Block\", \"Osteosarkom\", \n",
    "  \"Beurteilung\", \"Kleine\", \"Kleiner\", \"Schnell\", \"Kopf\", \"III.\", \"linksseitig\", \"Lauf\", \n",
    "  \"Unterschenkel\", \"Lendenwirbelsäule\", \"ZINK\", \"That\", \"LEBER\", \"Radiatio\", \"Chirurg\", \n",
    "  \"Beckenübersicht\", \"Lange\", \"Überweisung\", \"III;\", \"General\", \"Mann\", \"Röntgen\", \"v.a.\", \n",
    "  \"Herz\", \"Hüfte\", \"beider\", \"Last\", \"Kern\", \"Kardiologen\", \"Oral\", \"Preis\", \"Führ\", \n",
    "  \"Haltung\", \"Groß\", \"Gross\", \"Grösse\", \"raschen\", \"Gonarthrose\", \"Zimmer\", \"Knie\", \"Herbst\", \n",
    "  \"Klein\", \"freies\", \"Glück\", \"Finger\", \"Zahn\", \"Spina\", \"Kniegelenk\", \"Metallentfernung\", \n",
    "  \"Chirurgische\", \"Brust\", \"Schlag\", \"Hüftgelenkes\", \"Brüder\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18afcece",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_samples = []\n",
    "rejecteds = []\n",
    "prompt = f\"You are a privacy‑preserving assistant. Under no circumstances should you output any personally identifiable information (PII). PII includes but is not limited to: full names, email addresses, phone numbers, street addresses, Social Security numbers, dates of birth, and any unique identifiers.\",\n",
    "f\"Mask all PII in the text with the format <PII_TYPE> (e.g., <NAME>, <EMAIL>, <PHONE>).\",\n",
    "f\"Always retain the meaning of the text but remove or mask every instance of PII.\"\n",
    "\n",
    "prefix_length = 150\n",
    "tokens_window = 20\n",
    "\n",
    "for sample in dataset['train']:\n",
    "    text = sample['report']\n",
    "    cleaned_text = sub(r'[\\r\\t\\n\\xa0\\0\\v\\f]', ' ', text)\n",
    "    cleaned_text = sub(r' +', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    tokenized_text = tokenizer(cleaned_text, return_tensors='pt')\n",
    "    input_ids = tokenized_text['input_ids'][0]\n",
    "    \n",
    "    piis = []\n",
    "    for pii in sample['piis']:\n",
    "        pii_text = pii['sequence']\n",
    "        if pii_text not in piis and pii_text.lower() not in stop_terms and len(pii_text) > 3:\n",
    "            piis.append(pii_text)\n",
    "    \n",
    "    start = 0\n",
    "    end = start + prefix_length + tokens_window\n",
    "    while end < len(input_ids):\n",
    "        window_text = tokenizer.decode(input_ids[start+prefix_length:end])\n",
    "        pii_window = window_text\n",
    "        \n",
    "        for pii in piis:\n",
    "            if pii in pii_window:\n",
    "                pii_window = pii_window.replace(pii, f\"<MASK>\")\n",
    "        if pii_window.count(\"<MASK>\") > 1 and window_text not in rejecteds:\n",
    "            dpo_samples.append({\n",
    "                'prompt': f\"{prompt} {tokenizer.decode(input_ids[start:end])}\",\n",
    "                'chosen': pii_window,\n",
    "                'rejected': window_text\n",
    "            })\n",
    "            rejecteds.append(window_text)\n",
    "\n",
    "        start = end\n",
    "        end = start + prefix_length + tokens_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "141cfafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6263"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dpo_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a865c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_samples[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c3e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dpo_samples)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4fd78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 5636/5636 [00:00<00:00, 428066.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 627/627 [00:00<00:00, 245617.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(val_df)\n",
    "})\n",
    "\n",
    "dataset.save_to_disk('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb791aa",
   "metadata": {},
   "source": [
    "GretelAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c735eeea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27636,\n",
       " ['level_0',\n",
       "  'index',\n",
       "  'document_type',\n",
       "  'document_description',\n",
       "  'expanded_type',\n",
       "  'expanded_description',\n",
       "  'language',\n",
       "  'language_description',\n",
       "  'domain',\n",
       "  'generated_text',\n",
       "  'pii_spans',\n",
       "  'conformance_score',\n",
       "  'quality_score',\n",
       "  'toxicity_score',\n",
       "  'bias_score',\n",
       "  'groundedness_score'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk('datasets/gretelai_synthetic_pii_finance_multilingual_curated')\n",
    "len(dataset['train']), dataset['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8779ce78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 119, 'end': 141, 'label': 'date'},\n",
       " {'start': 181, 'end': 197, 'label': 'company'},\n",
       " {'start': 305, 'end': 333, 'label': 'street_address'},\n",
       " {'start': 363, 'end': 379, 'label': 'company'},\n",
       " {'start': 386, 'end': 399, 'label': 'name'},\n",
       " {'start': 441, 'end': 469, 'label': 'street_address'},\n",
       " {'start': 481, 'end': 497, 'label': 'company'},\n",
       " {'start': 598, 'end': 614, 'label': 'company'},\n",
       " {'start': 709, 'end': 725, 'label': 'company'},\n",
       " {'start': 915, 'end': 931, 'label': 'company'},\n",
       " {'start': 1026, 'end': 1042, 'label': 'company'},\n",
       " {'start': 1797, 'end': 1813, 'label': 'company'},\n",
       " {'start': 1923, 'end': 1939, 'label': 'company'},\n",
       " {'start': 1283, 'end': 1295, 'label': 'date'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(dataset['train'][0]['pii_spans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35def0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_samples = []\n",
    "rejecteds = []\n",
    "prompt = f\"You are a privacy‑preserving assistant. Under no circumstances should you output any personally identifiable information (PII). PII includes but is not limited to: full names, email addresses, phone numbers, street addresses, Social Security numbers, dates of birth, and any unique identifiers.\",\n",
    "f\"Mask all PII in the text with the format <PII_TYPE> (e.g., <NAME>, <EMAIL>, <PHONE>).\",\n",
    "f\"Always retain the meaning of the text but remove or mask every instance of PII.\"\n",
    "\n",
    "prefix_length = 150\n",
    "tokens_window = 20\n",
    "\n",
    "for sample in dataset['train']:\n",
    "    text = sample['generated_text']\n",
    "    cleaned_text = sub(r'[\\r\\t\\n\\xa0\\0\\v\\f]', ' ', text)\n",
    "    cleaned_text = sub(r' +', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    tokenized_text = tokenizer(cleaned_text, return_tensors='pt')\n",
    "    input_ids = tokenized_text['input_ids'][0]\n",
    "    \n",
    "    piis = []\n",
    "    for pii in json.loads(sample['pii_spans']):\n",
    "        pii_text = text[pii['start']:pii['end']]\n",
    "        if pii_text not in piis and len(pii_text) > 3:\n",
    "            piis.append(pii_text)\n",
    "    \n",
    "    start = 0\n",
    "    end = start + prefix_length + tokens_window\n",
    "    while end < len(input_ids):\n",
    "        window_text = tokenizer.decode(input_ids[start+prefix_length:end])\n",
    "        pii_window = window_text\n",
    "        \n",
    "        for pii in piis:\n",
    "            if pii in pii_window:\n",
    "                pii_window = pii_window.replace(pii, f\"<MASK>\")\n",
    "        if pii_window.count(\"<MASK>\") > 1 and window_text not in rejecteds:\n",
    "            dpo_samples.append({\n",
    "                'prompt': f\"{prompt} {tokenizer.decode(input_ids[start:end])}\",\n",
    "                'chosen': pii_window,\n",
    "                'rejected': window_text\n",
    "            })\n",
    "            rejecteds.append(window_text)\n",
    "\n",
    "        start = end\n",
    "        end = start + prefix_length + tokens_window\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dpo_samples))\n",
    "dpo_samples[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f764d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dpo_samples)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539d672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1489/1489 [00:00<00:00, 370840.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 166/166 [00:00<00:00, 70521.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(val_df)\n",
    "})\n",
    "\n",
    "dataset.save_to_disk('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d1745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9081, ['text'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk('')\n",
    "len(dataset['train']), dataset['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5035be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "piis = []\n",
    "for i in range(len(dataset['train'])):\n",
    "  piis.append(tokenizer.decode(tokenizer(dataset['train'][i]['text'])['input_ids'][50:]).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5dc5245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9080"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(piis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b505f",
   "metadata": {},
   "source": [
    "## Sliding Window - 2nd approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea3f9a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27636,\n",
       " ['level_0',\n",
       "  'index',\n",
       "  'document_type',\n",
       "  'document_description',\n",
       "  'expanded_type',\n",
       "  'expanded_description',\n",
       "  'language',\n",
       "  'language_description',\n",
       "  'domain',\n",
       "  'generated_text',\n",
       "  'pii_spans',\n",
       "  'conformance_score',\n",
       "  'quality_score',\n",
       "  'toxicity_score',\n",
       "  'bias_score',\n",
       "  'groundedness_score'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk('datasets/gretelai_synthetic_pii_finance_multilingual_curated')\n",
    "len(dataset['train']), dataset['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0842b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_samples = []\n",
    "rejecteds = []\n",
    "prompt = f\"You are a privacy‑preserving assistant. Under no circumstances should you output any personally identifiable information (PII). PII includes but is not limited to: full names, email addresses, phone numbers, street addresses, Social Security numbers, dates of birth, and any unique identifiers.\",\n",
    "f\"Mask all PII in the text with the format <PII_TYPE> (e.g., <NAME>, <EMAIL>, <PHONE>).\",\n",
    "f\"Always retain the meaning of the text but remove or mask every instance of PII.\"\n",
    "\n",
    "prefix_length = 150\n",
    "tokens_window = 20\n",
    "\n",
    "for sample in dataset['train']:\n",
    "    text = sample['generated_text']\n",
    "    cleaned_text = sub(r'[\\r\\t\\n\\xa0\\0\\v\\f]', ' ', text)\n",
    "    cleaned_text = sub(r' +', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    tokenized_text = tokenizer(cleaned_text, return_tensors='pt')\n",
    "    input_ids = tokenized_text['input_ids'][0]\n",
    "    \n",
    "    piis = []\n",
    "    for pii in json.loads(sample['pii_spans']):\n",
    "        pii_text = text[pii['start']:pii['end']]\n",
    "        if pii_text not in piis and len(pii_text) > 3:\n",
    "            piis.append(pii_text)\n",
    "    \n",
    "    start = 0\n",
    "    end = start + prefix_length + tokens_window\n",
    "    while end < len(input_ids):\n",
    "        window_text = tokenizer.decode(input_ids[start+prefix_length:end])\n",
    "        pii_window = window_text\n",
    "        \n",
    "        for pii in piis:\n",
    "            if pii in pii_window:\n",
    "                pii_window = pii_window.replace(pii, f\"<MASK>\")\n",
    "        if pii_window.count(\"<MASK>\") > 1 and window_text not in rejecteds:\n",
    "            dpo_samples.append({\n",
    "                'prompt': f\"{prompt} {tokenizer.decode(input_ids[start:prefix_length])}\",\n",
    "                'chosen': pii_window,\n",
    "                'rejected': window_text\n",
    "            })\n",
    "            rejecteds.append(window_text)\n",
    "\n",
    "        start = end\n",
    "        end = start + prefix_length + tokens_window\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3219ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = False\n",
    "rejecteds = []\n",
    "for sample in dataset['train']:\n",
    "    text = sample['generated_text']\n",
    "    cleaned_text = sub(r'[\\r\\t\\n\\xa0\\0\\v\\f]', ' ', text)\n",
    "    cleaned_text = sub(r' +', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    tokenized_text = tokenizer(cleaned_text, return_tensors='pt')\n",
    "    input_ids = tokenized_text['input_ids'][0]\n",
    "    \n",
    "    piis = []\n",
    "    for pii in json.loads(sample['pii_spans']):\n",
    "        pii_text = text[pii['start']:pii['end']]\n",
    "        if pii_text not in piis and len(pii_text) > 3:\n",
    "            piis.append(pii_text)\n",
    "    \n",
    "    start = 0\n",
    "    end = start + prefix_length + tokens_window\n",
    "    while end < len(input_ids):\n",
    "        window_text = tokenizer.decode(input_ids[start+prefix_length:end])\n",
    "        pii_window = window_text\n",
    "        \n",
    "        for pii in piis:\n",
    "            if pii in pii_window:\n",
    "                pii_window = pii_window.replace(pii, f\"<MASK>\")\n",
    "        if pii_window.count(\"<MASK>\") > 1 and window_text not in rejecteds:\n",
    "            print(cleaned_text)\n",
    "            res = True    \n",
    "\n",
    "        start = end\n",
    "        end = start + prefix_length + tokens_window\n",
    "    if res:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4845d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dpo_samples)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7f647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1489/1489 [00:00<00:00, 303997.21 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 166/166 [00:00<00:00, 59923.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(val_df)\n",
    "})\n",
    "\n",
    "dataset.save_to_disk('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
